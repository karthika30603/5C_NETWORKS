{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NtmhG0AhKY3T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d513d13a-11b7-4a6a-af92-d2991891be0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Apply CLAHE\n",
        "def apply_clahe(image):\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    return clahe.apply(image)\n",
        "\n",
        "# Custom Dataset Class for 3D Brain MRI\n",
        "class Metastasis3DDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.image_filenames = []\n",
        "        self.mask_filenames = []\n",
        "\n",
        "        # Traverse through subdirectories and collect all .tif files\n",
        "        for root, _, files in os.walk(data_dir):\n",
        "            for f in files:\n",
        "                # Skip __MACOSX and ._ files\n",
        "                if \"__MACOSX\" in root or f.startswith(\"._\"):\n",
        "                    continue\n",
        "                if f.endswith('.tif'):\n",
        "                    if '_mask' in f:  # Check if the file is a mask\n",
        "                        self.mask_filenames.append(os.path.join(root, f))\n",
        "                    else:  # Otherwise, it's an image\n",
        "                        self.image_filenames.append(os.path.join(root, f))\n",
        "\n",
        "        # Keep only the matched pairs\n",
        "        matched_images = set()\n",
        "        matched_masks = set()\n",
        "\n",
        "        for mask in self.mask_filenames:\n",
        "            mask_name = os.path.basename(mask).replace('_mask.tif', '.tif')\n",
        "            image_path = os.path.join(os.path.dirname(mask), mask_name)\n",
        "            if image_path in self.image_filenames:\n",
        "                matched_images.add(image_path)\n",
        "                matched_masks.add(mask)\n",
        "\n",
        "        self.image_filenames = list(matched_images)\n",
        "        self.mask_filenames = list(matched_masks)\n",
        "\n",
        "        # Ensure that the number of images and masks match\n",
        "        if len(self.image_filenames) != len(self.mask_filenames):\n",
        "            print(f\"Warning: {len(self.image_filenames)} images and {len(self.mask_filenames)} masks found.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_filenames[idx]\n",
        "        mask_path = self.mask_filenames[idx]  # Get the corresponding mask\n",
        "\n",
        "        # Load and preprocess the image\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if image is None:\n",
        "            raise FileNotFoundError(f\"Image not found at {img_path}\")\n",
        "\n",
        "        image = apply_clahe(image)  # Apply CLAHE\n",
        "        image = image / 255.0  # Normalize pixel values\n",
        "\n",
        "        # Load the mask\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if mask is None:\n",
        "            raise FileNotFoundError(f\"Mask not found at {mask_path}\")\n",
        "\n",
        "        mask = mask / 255.0  # Normalize mask pixel values\n",
        "\n",
        "        # Convert to a PIL Image for transformation\n",
        "        image = Image.fromarray((image * 255).astype(np.uint8))  # Convert to PIL Image\n",
        "        mask = Image.fromarray((mask * 255).astype(np.uint8))  # Convert to PIL Image\n",
        "\n",
        "        # Data Augmentation and Transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# Define Data Augmentation and Normalization\n",
        "def get_transform(phase):\n",
        "    if phase == 'train':\n",
        "        return transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "    else:\n",
        "        return transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    data_dir = \"/content/drive/MyDrive/Data\"  # Update this to your actual data directory\n",
        "    train_dataset = Metastasis3DDataset(data_dir=data_dir, transform=get_transform('train'))\n",
        "\n",
        "    # Create DataLoader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    # Test DataLoader\n",
        "    for images, masks in train_loader:\n",
        "        print(images.shape, masks.shape)  # Verify the shape of the images and masks\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfJDxCI4ROPm",
        "outputId": "d7d0734b-987f-4a8d-ab22-c834ff9791d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 256, 256]) torch.Size([1, 1, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Attention Gate\n",
        "class AttentionGate(nn.Module):\n",
        "    def __init__(self, F_g, F_l, F_int):\n",
        "        super(AttentionGate, self).__init__()\n",
        "        self.W_g = nn.Conv2d(F_g, F_int, kernel_size=1, padding=0)\n",
        "        self.W_x = nn.Conv2d(F_l, F_int, kernel_size=1, padding=0)\n",
        "        self.psi = nn.Conv2d(F_int, 1, kernel_size=1, padding=0)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        g1 = self.W_g(g)\n",
        "        x1 = self.W_x(x)\n",
        "        psi = self.relu(g1 + x1)\n",
        "        psi = self.psi(psi)\n",
        "        attention = torch.sigmoid(psi)\n",
        "        return x * attention\n",
        "\n",
        "# Basic Convolution Block\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        return x\n",
        "\n",
        "# Up-sampling block for decoding path\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UpBlock, self).__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "        self.conv = ConvBlock(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat((x, skip), dim=1)  # Concatenate with the skip connection\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "# Attention U-Net Architecture\n",
        "class AttentionUNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, num_classes=1):\n",
        "        super(AttentionUNet, self).__init__()\n",
        "\n",
        "        # Encoder Path\n",
        "        self.conv1 = ConvBlock(in_channels, 64)   # Level 1\n",
        "        self.conv2 = ConvBlock(64, 128)            # Level 2\n",
        "        self.conv3 = ConvBlock(128, 256)           # Level 3\n",
        "        self.conv4 = ConvBlock(256, 512)           # Level 4\n",
        "        self.conv5 = ConvBlock(512, 1024)          # Level 5\n",
        "\n",
        "        # Decoder Path\n",
        "        self.up4 = UpBlock(1024, 512)              # Level 4\n",
        "        self.up3 = UpBlock(512, 256)                # Level 3\n",
        "        self.up2 = UpBlock(256, 128)                # Level 2\n",
        "        self.up1 = UpBlock(128, 64)                 # Level 1\n",
        "\n",
        "        # Attention Gates\n",
        "        self.att4 = AttentionGate(F_g=512, F_l=512, F_int=256)\n",
        "        self.att3 = AttentionGate(F_g=256, F_l=256, F_int=128)\n",
        "        self.att2 = AttentionGate(F_g=128, F_l=128, F_int=64)\n",
        "        self.att1 = AttentionGate(F_g=64, F_l=64, F_int=32)\n",
        "\n",
        "        # Final Convolution\n",
        "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x1 = self.conv1(x)   # Level 1\n",
        "        x2 = self.conv2(F.max_pool2d(x1, 2))  # Level 2\n",
        "        x3 = self.conv3(F.max_pool2d(x2, 2))  # Level 3\n",
        "        x4 = self.conv4(F.max_pool2d(x3, 2))  # Level 4\n",
        "        x5 = self.conv5(F.max_pool2d(x4, 2))  # Level 5\n",
        "\n",
        "        # Decoder with Attention Gates\n",
        "        x4_up = self.up4(x5, x4)\n",
        "        x4_att = self.att4(x4_up, x4)\n",
        "\n",
        "        x3_up = self.up3(x4_att, x3)\n",
        "        x3_att = self.att3(x3_up, x3)\n",
        "\n",
        "        x2_up = self.up2(x3_att, x2)\n",
        "        x2_att = self.att2(x2_up, x2)\n",
        "\n",
        "        x1_up = self.up1(x2_att, x1)\n",
        "        x1_att = self.att1(x1_up, x1)\n",
        "\n",
        "        # Output\n",
        "        output = self.final_conv(x1_att)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Example of creating a model\n",
        "model = AttentionUNet(in_channels=1, num_classes=1)\n"
      ],
      "metadata": {
        "id": "buD9QDAVRbk-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize the model and move it to GPU if available\n",
        "model = AttentionUNet(in_channels=1, num_classes=1).to(device)\n",
        "\n",
        "# Define loss function (Dice Loss is commonly used in segmentation)\n",
        "def dice_loss(pred, target, smooth=1):\n",
        "    pred = torch.sigmoid(pred)  # Sigmoid activation for binary segmentation\n",
        "    pred_flat = pred.view(-1)\n",
        "    target_flat = target.view(-1)\n",
        "    intersection = (pred_flat * target_flat).sum()\n",
        "    return 1 - ((2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth))\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# DataLoader (assuming dataset is already defined and loaded)\n",
        "data_dir = \"/content/drive/MyDrive/Data\"  # Update this with the correct path\n",
        "train_dataset = Metastasis3DDataset(data_dir=data_dir, transform=get_transform('train'))\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
        "\n",
        "# Training settings\n",
        "num_epochs = 4  # Adjust as needed\n",
        "accumulation_steps = 4  # Gradient accumulation steps to simulate larger batches\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    print(f'Starting epoch {epoch + 1}/{num_epochs}...')\n",
        "\n",
        "    for batch_idx, (images, masks) in enumerate(train_loader):\n",
        "        # Move images and masks to GPU (if available)\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate Dice loss\n",
        "        loss = dice_loss(outputs, masks.unsqueeze(1))\n",
        "\n",
        "        # Normalize loss for gradient accumulation\n",
        "        loss = loss / accumulation_steps\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights after accumulating gradients for several steps\n",
        "        if (batch_idx + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "\n",
        "        # Accumulate the loss for monitoring\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Print progress every 10 batches\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Average loss for the epoch\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}] finished with average loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"Training completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhK4ZemRRuVf",
        "outputId": "feed64bf-82c8-47b0-d4ab-68ee0b78c3a2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Starting epoch 1/4...\n",
            "Epoch [1/4], Batch [0/901], Loss: 0.2372\n",
            "Epoch [1/4], Batch [10/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [20/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [30/901], Loss: 0.2390\n",
            "Epoch [1/4], Batch [40/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [50/901], Loss: 0.2256\n",
            "Epoch [1/4], Batch [60/901], Loss: 0.2380\n",
            "Epoch [1/4], Batch [70/901], Loss: 0.2101\n",
            "Epoch [1/4], Batch [80/901], Loss: 0.2439\n",
            "Epoch [1/4], Batch [90/901], Loss: 0.2016\n",
            "Epoch [1/4], Batch [100/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [110/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [120/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [130/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [140/901], Loss: 0.2367\n",
            "Epoch [1/4], Batch [150/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [160/901], Loss: 0.2071\n",
            "Epoch [1/4], Batch [170/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [180/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [190/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [200/901], Loss: 0.2115\n",
            "Epoch [1/4], Batch [210/901], Loss: 0.2446\n",
            "Epoch [1/4], Batch [220/901], Loss: 0.2110\n",
            "Epoch [1/4], Batch [230/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [240/901], Loss: 0.2471\n",
            "Epoch [1/4], Batch [250/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [260/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [270/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [280/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [290/901], Loss: 0.2277\n",
            "Epoch [1/4], Batch [300/901], Loss: 0.2360\n",
            "Epoch [1/4], Batch [310/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [320/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [330/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [340/901], Loss: 0.2342\n",
            "Epoch [1/4], Batch [350/901], Loss: 0.2392\n",
            "Epoch [1/4], Batch [360/901], Loss: 0.2315\n",
            "Epoch [1/4], Batch [370/901], Loss: 0.2424\n",
            "Epoch [1/4], Batch [380/901], Loss: 0.2445\n",
            "Epoch [1/4], Batch [390/901], Loss: 0.2088\n",
            "Epoch [1/4], Batch [400/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [410/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [420/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [430/901], Loss: 0.2350\n",
            "Epoch [1/4], Batch [440/901], Loss: 0.2300\n",
            "Epoch [1/4], Batch [450/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [460/901], Loss: 0.2486\n",
            "Epoch [1/4], Batch [470/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [480/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [490/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [500/901], Loss: 0.2296\n",
            "Epoch [1/4], Batch [510/901], Loss: 0.2278\n",
            "Epoch [1/4], Batch [520/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [530/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [540/901], Loss: 0.2326\n",
            "Epoch [1/4], Batch [550/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [560/901], Loss: 0.2445\n",
            "Epoch [1/4], Batch [570/901], Loss: 0.2286\n",
            "Epoch [1/4], Batch [580/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [590/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [600/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [610/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [620/901], Loss: 0.2361\n",
            "Epoch [1/4], Batch [630/901], Loss: 0.2371\n",
            "Epoch [1/4], Batch [640/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [650/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [660/901], Loss: 0.2461\n",
            "Epoch [1/4], Batch [670/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [680/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [690/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [700/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [710/901], Loss: 0.2298\n",
            "Epoch [1/4], Batch [720/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [730/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [740/901], Loss: 0.2394\n",
            "Epoch [1/4], Batch [750/901], Loss: 0.2383\n",
            "Epoch [1/4], Batch [760/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [770/901], Loss: 0.2431\n",
            "Epoch [1/4], Batch [780/901], Loss: 0.2463\n",
            "Epoch [1/4], Batch [790/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [800/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [810/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [820/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [830/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [840/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [850/901], Loss: 0.2047\n",
            "Epoch [1/4], Batch [860/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [870/901], Loss: 0.2447\n",
            "Epoch [1/4], Batch [880/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [890/901], Loss: 0.2500\n",
            "Epoch [1/4], Batch [900/901], Loss: 0.2500\n",
            "Epoch [1/4] finished with average loss: 0.2436\n",
            "Starting epoch 2/4...\n",
            "Epoch [2/4], Batch [0/901], Loss: 0.2368\n",
            "Epoch [2/4], Batch [10/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [20/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [30/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [40/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [50/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [60/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [70/901], Loss: 0.2474\n",
            "Epoch [2/4], Batch [80/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [90/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [100/901], Loss: 0.2360\n",
            "Epoch [2/4], Batch [110/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [120/901], Loss: 0.2403\n",
            "Epoch [2/4], Batch [130/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [140/901], Loss: 0.2329\n",
            "Epoch [2/4], Batch [150/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [160/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [170/901], Loss: 0.2400\n",
            "Epoch [2/4], Batch [180/901], Loss: 0.2276\n",
            "Epoch [2/4], Batch [190/901], Loss: 0.2228\n",
            "Epoch [2/4], Batch [200/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [210/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [220/901], Loss: 0.2248\n",
            "Epoch [2/4], Batch [230/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [240/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [250/901], Loss: 0.2219\n",
            "Epoch [2/4], Batch [260/901], Loss: 0.2307\n",
            "Epoch [2/4], Batch [270/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [280/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [290/901], Loss: 0.2403\n",
            "Epoch [2/4], Batch [300/901], Loss: 0.1897\n",
            "Epoch [2/4], Batch [310/901], Loss: 0.2236\n",
            "Epoch [2/4], Batch [320/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [330/901], Loss: 0.2175\n",
            "Epoch [2/4], Batch [340/901], Loss: 0.1756\n",
            "Epoch [2/4], Batch [350/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [360/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [370/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [380/901], Loss: 0.2215\n",
            "Epoch [2/4], Batch [390/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [400/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [410/901], Loss: 0.2173\n",
            "Epoch [2/4], Batch [420/901], Loss: 0.2494\n",
            "Epoch [2/4], Batch [430/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [440/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [450/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [460/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [470/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [480/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [490/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [500/901], Loss: 0.2196\n",
            "Epoch [2/4], Batch [510/901], Loss: 0.2244\n",
            "Epoch [2/4], Batch [520/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [530/901], Loss: 0.2084\n",
            "Epoch [2/4], Batch [540/901], Loss: 0.2317\n",
            "Epoch [2/4], Batch [550/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [560/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [570/901], Loss: 0.2348\n",
            "Epoch [2/4], Batch [580/901], Loss: 0.2415\n",
            "Epoch [2/4], Batch [590/901], Loss: 0.2367\n",
            "Epoch [2/4], Batch [600/901], Loss: 0.2186\n",
            "Epoch [2/4], Batch [610/901], Loss: 0.2300\n",
            "Epoch [2/4], Batch [620/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [630/901], Loss: 0.2411\n",
            "Epoch [2/4], Batch [640/901], Loss: 0.2405\n",
            "Epoch [2/4], Batch [650/901], Loss: 0.2130\n",
            "Epoch [2/4], Batch [660/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [670/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [680/901], Loss: 0.2392\n",
            "Epoch [2/4], Batch [690/901], Loss: 0.2288\n",
            "Epoch [2/4], Batch [700/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [710/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [720/901], Loss: 0.2245\n",
            "Epoch [2/4], Batch [730/901], Loss: 0.2467\n",
            "Epoch [2/4], Batch [740/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [750/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [760/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [770/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [780/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [790/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [800/901], Loss: 0.2380\n",
            "Epoch [2/4], Batch [810/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [820/901], Loss: 0.2041\n",
            "Epoch [2/4], Batch [830/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [840/901], Loss: 0.2238\n",
            "Epoch [2/4], Batch [850/901], Loss: 0.2324\n",
            "Epoch [2/4], Batch [860/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [870/901], Loss: 0.2220\n",
            "Epoch [2/4], Batch [880/901], Loss: 0.2083\n",
            "Epoch [2/4], Batch [890/901], Loss: 0.2500\n",
            "Epoch [2/4], Batch [900/901], Loss: 0.2500\n",
            "Epoch [2/4] finished with average loss: 0.2393\n",
            "Starting epoch 3/4...\n",
            "Epoch [3/4], Batch [0/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [10/901], Loss: 0.2251\n",
            "Epoch [3/4], Batch [20/901], Loss: 0.2136\n",
            "Epoch [3/4], Batch [30/901], Loss: 0.2496\n",
            "Epoch [3/4], Batch [40/901], Loss: 0.1863\n",
            "Epoch [3/4], Batch [50/901], Loss: 0.2172\n",
            "Epoch [3/4], Batch [60/901], Loss: 0.1502\n",
            "Epoch [3/4], Batch [70/901], Loss: 0.2261\n",
            "Epoch [3/4], Batch [80/901], Loss: 0.1442\n",
            "Epoch [3/4], Batch [90/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [100/901], Loss: 0.2156\n",
            "Epoch [3/4], Batch [110/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [120/901], Loss: 0.2357\n",
            "Epoch [3/4], Batch [130/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [140/901], Loss: 0.2148\n",
            "Epoch [3/4], Batch [150/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [160/901], Loss: 0.2206\n",
            "Epoch [3/4], Batch [170/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [180/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [190/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [200/901], Loss: 0.2056\n",
            "Epoch [3/4], Batch [210/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [220/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [230/901], Loss: 0.1830\n",
            "Epoch [3/4], Batch [240/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [250/901], Loss: 0.2415\n",
            "Epoch [3/4], Batch [260/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [270/901], Loss: 0.2455\n",
            "Epoch [3/4], Batch [280/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [290/901], Loss: 0.2328\n",
            "Epoch [3/4], Batch [300/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [310/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [320/901], Loss: 0.2470\n",
            "Epoch [3/4], Batch [330/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [340/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [350/901], Loss: 0.2304\n",
            "Epoch [3/4], Batch [360/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [370/901], Loss: 0.2296\n",
            "Epoch [3/4], Batch [380/901], Loss: 0.2149\n",
            "Epoch [3/4], Batch [390/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [400/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [410/901], Loss: 0.2207\n",
            "Epoch [3/4], Batch [420/901], Loss: 0.2464\n",
            "Epoch [3/4], Batch [430/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [440/901], Loss: 0.2441\n",
            "Epoch [3/4], Batch [450/901], Loss: 0.1417\n",
            "Epoch [3/4], Batch [460/901], Loss: 0.2065\n",
            "Epoch [3/4], Batch [470/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [480/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [490/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [500/901], Loss: 0.2416\n",
            "Epoch [3/4], Batch [510/901], Loss: 0.2234\n",
            "Epoch [3/4], Batch [520/901], Loss: 0.2330\n",
            "Epoch [3/4], Batch [530/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [540/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [550/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [560/901], Loss: 0.2067\n",
            "Epoch [3/4], Batch [570/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [580/901], Loss: 0.2433\n",
            "Epoch [3/4], Batch [590/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [600/901], Loss: 0.2473\n",
            "Epoch [3/4], Batch [610/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [620/901], Loss: 0.2498\n",
            "Epoch [3/4], Batch [630/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [640/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [650/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [660/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [670/901], Loss: 0.2475\n",
            "Epoch [3/4], Batch [680/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [690/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [700/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [710/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [720/901], Loss: 0.2258\n",
            "Epoch [3/4], Batch [730/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [740/901], Loss: 0.2247\n",
            "Epoch [3/4], Batch [750/901], Loss: 0.2197\n",
            "Epoch [3/4], Batch [760/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [770/901], Loss: 0.2156\n",
            "Epoch [3/4], Batch [780/901], Loss: 0.2465\n",
            "Epoch [3/4], Batch [790/901], Loss: 0.2191\n",
            "Epoch [3/4], Batch [800/901], Loss: 0.1891\n",
            "Epoch [3/4], Batch [810/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [820/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [830/901], Loss: 0.2293\n",
            "Epoch [3/4], Batch [840/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [850/901], Loss: 0.2256\n",
            "Epoch [3/4], Batch [860/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [870/901], Loss: 0.2018\n",
            "Epoch [3/4], Batch [880/901], Loss: 0.2500\n",
            "Epoch [3/4], Batch [890/901], Loss: 0.2471\n",
            "Epoch [3/4], Batch [900/901], Loss: 0.2170\n",
            "Epoch [3/4] finished with average loss: 0.2372\n",
            "Starting epoch 4/4...\n",
            "Epoch [4/4], Batch [0/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [10/901], Loss: 0.2154\n",
            "Epoch [4/4], Batch [20/901], Loss: 0.2499\n",
            "Epoch [4/4], Batch [30/901], Loss: 0.2343\n",
            "Epoch [4/4], Batch [40/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [50/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [60/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [70/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [80/901], Loss: 0.1865\n",
            "Epoch [4/4], Batch [90/901], Loss: 0.1697\n",
            "Epoch [4/4], Batch [100/901], Loss: 0.2327\n",
            "Epoch [4/4], Batch [110/901], Loss: 0.2250\n",
            "Epoch [4/4], Batch [120/901], Loss: 0.2248\n",
            "Epoch [4/4], Batch [130/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [140/901], Loss: 0.2266\n",
            "Epoch [4/4], Batch [150/901], Loss: 0.2471\n",
            "Epoch [4/4], Batch [160/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [170/901], Loss: 0.1924\n",
            "Epoch [4/4], Batch [180/901], Loss: 0.2484\n",
            "Epoch [4/4], Batch [190/901], Loss: 0.2098\n",
            "Epoch [4/4], Batch [200/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [210/901], Loss: 0.2324\n",
            "Epoch [4/4], Batch [220/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [230/901], Loss: 0.2445\n",
            "Epoch [4/4], Batch [240/901], Loss: 0.2145\n",
            "Epoch [4/4], Batch [250/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [260/901], Loss: 0.2094\n",
            "Epoch [4/4], Batch [270/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [280/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [290/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [300/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [310/901], Loss: 0.2394\n",
            "Epoch [4/4], Batch [320/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [330/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [340/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [350/901], Loss: 0.2405\n",
            "Epoch [4/4], Batch [360/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [370/901], Loss: 0.2141\n",
            "Epoch [4/4], Batch [380/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [390/901], Loss: 0.2370\n",
            "Epoch [4/4], Batch [400/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [410/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [420/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [430/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [440/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [450/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [460/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [470/901], Loss: 0.2131\n",
            "Epoch [4/4], Batch [480/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [490/901], Loss: 0.2331\n",
            "Epoch [4/4], Batch [500/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [510/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [520/901], Loss: 0.2499\n",
            "Epoch [4/4], Batch [530/901], Loss: 0.1878\n",
            "Epoch [4/4], Batch [540/901], Loss: 0.2307\n",
            "Epoch [4/4], Batch [550/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [560/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [570/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [580/901], Loss: 0.2027\n",
            "Epoch [4/4], Batch [590/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [600/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [610/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [620/901], Loss: 0.1430\n",
            "Epoch [4/4], Batch [630/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [640/901], Loss: 0.2499\n",
            "Epoch [4/4], Batch [650/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [660/901], Loss: 0.2464\n",
            "Epoch [4/4], Batch [670/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [680/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [690/901], Loss: 0.2142\n",
            "Epoch [4/4], Batch [700/901], Loss: 0.2142\n",
            "Epoch [4/4], Batch [710/901], Loss: 0.2385\n",
            "Epoch [4/4], Batch [720/901], Loss: 0.2083\n",
            "Epoch [4/4], Batch [730/901], Loss: 0.1599\n",
            "Epoch [4/4], Batch [740/901], Loss: 0.2498\n",
            "Epoch [4/4], Batch [750/901], Loss: 0.2294\n",
            "Epoch [4/4], Batch [760/901], Loss: 0.2052\n",
            "Epoch [4/4], Batch [770/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [780/901], Loss: 0.1839\n",
            "Epoch [4/4], Batch [790/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [800/901], Loss: 0.2421\n",
            "Epoch [4/4], Batch [810/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [820/901], Loss: 0.2190\n",
            "Epoch [4/4], Batch [830/901], Loss: 0.2312\n",
            "Epoch [4/4], Batch [840/901], Loss: 0.2486\n",
            "Epoch [4/4], Batch [850/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [860/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [870/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [880/901], Loss: 0.2500\n",
            "Epoch [4/4], Batch [890/901], Loss: 0.1922\n",
            "Epoch [4/4], Batch [900/901], Loss: 0.2500\n",
            "Epoch [4/4] finished with average loss: 0.2367\n",
            "Training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(model, val_loader, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_dice_score = 0\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Resize the mask to match the output\n",
        "            masks = masks.squeeze(1)  # Remove the extra channel dimension\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, masks)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate DICE score\n",
        "            outputs = torch.sigmoid(outputs)  # Apply sigmoid to get probabilities\n",
        "            preds = (outputs > 0.5).float()  # Binarize predictions\n",
        "\n",
        "            # Dice score calculation\n",
        "            intersection = (preds * masks).sum((1, 2, 3))\n",
        "            union = preds.sum((1, 2, 3)) + masks.sum((1, 2, 3))\n",
        "            dice_score = (2 * intersection + 1e-8) / (union + 1e-8)\n",
        "            total_dice_score += dice_score.mean().item()\n",
        "\n",
        "            num_batches += 1\n",
        "\n",
        "    avg_dice_score = total_dice_score / num_batches\n",
        "    avg_loss = total_loss / num_batches\n",
        "    print(avg_dice_score, avg_loss)\n",
        "    return avg_dice_score, avg_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "4V0oSmG7xojm"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}